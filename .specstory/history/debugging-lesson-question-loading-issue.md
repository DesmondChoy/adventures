## SpecStory

## Debugging Lesson Question Loading Issue (1/22/2025, 8:40:52 PM)

_**User**_

The functionality of this app is so at depth=1, the LLM tells a story and weaves in questions from load_lesson_data(). However, right now the questions at the end of depth=1 are self-generated.

---

_**Assistant**_

Okay, I understand. Let's investigate why the questions at depth=1 are not being pulled from `lessons.csv` as expected and are instead being self-generated by the LLM.

Based on your description and the conversation history, the issue seems to be within the `generate_story_segment` function in `app/main.py`. Let's examine the relevant part of the code again to pinpoint the problem.


```179:224:app/main.py
async def generate_story_segment(
    story_category: str, lesson_topic: str, state: StoryState
) -> StoryNode:
    """Generate the next story segment based on the current state."""
    # Load story configuration
    story_data = load_story_data()
    story_config = story_data["story_categories"][story_category]

    # Load lesson data if we're at the question depth
    question = None
    if state.depth == 1:  # Next node will be depth 2
        lesson_data = load_lesson_data()
        relevant_lessons = lesson_data[lesson_data["topic"] == lesson_topic]
        question = relevant_lessons.iloc[state.total_questions].to_dict()

    # Generate story content using LLM
    story_stream = await llm_service.generate_story_segment(
        story_config, state, question
    )

    # Process the streaming response
    story_content = ""
    story_choices = []

    try:
        async for chunk in story_stream:
            if chunk.choices[0].delta.content:
                story_content += chunk.choices[0].delta.content
    except Exception as e:
        print(f"Error during story generation: {e}")
        story_content = "I apologize, but I encountered an error generating the story. Please try again."

    # Extract choices based on whether this is a question node or not
    if question:
        story_choices = [
            StoryChoice(text=question["correct_answer"], next_node="correct"),
            StoryChoice(text=question["wrong_answer1"], next_node="wrong1"),
            StoryChoice(text=question["wrong_answer2"], next_node="wrong2"),
        ]
    else:
        story_choices = [
            StoryChoice(text=f"Choice {i + 1}", next_node=f"node_{state.depth}_{i}")
            for i in range(2)
        ]
    return StoryNode(content=story_content, choices=story_choices)
```


Looking at this code block, specifically lines 187-193, it seems like the code *is* attempting to load lesson data and extract a question when `state.depth == 1`.  It selects a lesson based on `lesson_topic` and then takes the `state.total_questions`-th row from the `relevant_lessons` DataFrame.  It then uses the `correct_answer`, `wrong_answer1`, and `wrong_answer2` from this question to create `story_choices`.

However, based on your feedback, the LLM output at depth=1 is still showing self-generated choices. This suggests one of the following possibilities:

1.  **The `question` variable is not being populated correctly**:  Perhaps there's an issue with how `relevant_lessons` is being filtered or how `iloc[state.total_questions]` is accessing the data. It's possible that `question` is `None` or doesn't contain the expected question data.
2.  **The `question` data is not being used in the LLM prompt**: Even if the `question` is loaded correctly, it might not be properly passed to the `llm_service.generate_story_segment` function or incorporated into the prompt in a way that forces the LLM to use the provided choices.
3.  **The LLM prompt is not strict enough**:  The prompt might be allowing the LLM to generate its own choices even when a question and choices are provided.

To further diagnose this, let's re-examine the `generate_story_segment` function and the `_build_user_prompt` function in `app/services/llm.py`.

First, let's look at the `_build_user_prompt` function to see how it handles the `question` data:


```85:118:app/services/llm.py
    def _build_user_prompt(
        self,
        story_config: Dict[str, Any],
        state: StoryState,
        question: Optional[Dict[str, Any]],
    ) -> str:
        """Create a user prompt that includes story state and current requirements."""
        prompt = f"""Current story state:
- Depth: {state.depth} of 3
- Previous choices: {", ".join(state.history) if state.history else "Story beginning"}

"""

        if state.depth == 0:
            prompt += "Generate the opening scene of the story, introducing the setting and main character."
        elif question:
            # Integrate educational question naturally into the story
            prompt += f"""Continue the story, naturally leading to a situation where the following question 
            can be asked: {question["question"]}

The choices should be:
1. {question["correct_answer"]}
2. {question["wrong_answer1"]}
3. {question["wrong_answer2"]}

The question should feel like a natural part of the story's progression."""
        else:
            # Regular story continuation
            prompt += """Continue the story based on the previous choices, 
            creating meaningful consequences for the character's decisions.
            
End this segment with two clear choices that would lead the story in different directions."""

        return prompt
```


In this code block, lines 100-110 show that when a `question` is provided, the prompt *should* be instructing the LLM to incorporate the question and use the provided choices.  It constructs a prompt that includes the question and lists the `correct_answer`, `wrong_answer1`, and `wrong_answer2` as the choices.

Given that both `generate_story_segment` and `_build_user_prompt` seem to be set up to use the database questions, the issue might be more subtle.

To proceed with debugging, I recommend the following steps:

1.  **Add more detailed logging in `generate_story_segment`**: Let's add logging to print the `question` dictionary right after it's loaded from `relevant_lessons`. This will help us confirm if the question data is being loaded correctly and what it contains.

    
```187:193:app/main.py
    # Load lesson data if we're at the question depth
    question = None
    if state.depth == 1:  # Next node will be depth 2
        lesson_data = load_lesson_data()
        relevant_lessons = lesson_data[lesson_data["topic"] == lesson_topic]
        question = relevant_lessons.iloc[state.total_questions].to_dict()

```

    Modify this section to include logging:

    
```187:193:app/main.py
    # Load lesson data if we're at the question depth
    question = None
    if state.depth == 1:  # Next node will be depth 2
        lesson_data = load_lesson_data()
        relevant_lessons = lesson_data[lesson_data["topic"] == lesson_topic]
        question = relevant_lessons.iloc[state.total_questions].to_dict()

```

    ```python
    if state.depth == 1:  # Next node will be depth 2
        lesson_data = load_lesson_data()
        relevant_lessons = lesson_data[lesson_data["topic"] == lesson_topic]
        question = relevant_lessons.iloc[state.total_questions].to_dict()
        print(f"\nDEBUG: Loaded question at depth=1: {question}") # ADDED DEBUG LOG
    ```

2.  **Run the story again with "Singapore History" and check the output**: After adding the debug log, run the story, select "Singapore History", and proceed to depth=1.  Examine the console output to see the "DEBUG: Loaded question at depth=1:" log.  Check if the logged `question` dictionary contains the expected question, correct answer, and wrong answers from your `lessons.csv` file.

3.  **Verify `lessons.csv` data**: Double-check your `lessons.csv` file to ensure it contains data for "Singapore History" and that the column names (`question`, `correct_answer`, `wrong_answer1`, `wrong_answer2`) match what the code expects.

By adding this logging and carefully examining the output, we should be able to pinpoint whether the issue is in loading the question data itself or in how it's being used later in the process.

Let me know the output of the debug log after you run the story again. This will give us more information to diagnose the problem.

---

_**User**_

I see main.py has a defintion for generate_story_segment(), and so does llm.py. Will that cause any conflicts?

---

_**Assistant**_

Yes, you've spotted a potential point of confusion!  You are correct that there are indeed two functions named `ge

---



---

