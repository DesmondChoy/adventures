# Implementation Plan: Two-Step Image Prompt Generation

## Implementation Status

**Status: NOT STARTED âŒ**

## 1. Problem Statement

The current image generation process struggles to maintain visual stability for the protagonist across different chapters. While agency details are included, combining these details with a base protagonist look and the specific scene description of the current chapter in a way that produces coherent images is challenging for a direct template-based approach.

Specifically:
*   **Inconsistent Integration:** Simple prompt concatenation (Scene + Protagonist + Agency) might lead to the image model treating elements separately rather than integrated (e.g., armor appearing *next to* the character instead of *on* them).
*   **Conflicting Details:** The base protagonist description (e.g., "wearing a blue dress") might conflict with agency visuals (e.g., "gleaming silver armor"). The template lacks the nuance to resolve this logically.
*   **Context Loss:** Relying solely on Imagen to interpret and combine multiple distinct descriptive elements within a single prompt can lead to some elements being ignored or misinterpreted depending on the specific chapter scene.

## 2. Root Cause Analysis

*   The `ImageGenerationService.enhance_prompt` function currently uses a template to combine the chapter's visual scene description and the stored agency details.
*   This template-based approach lacks the semantic understanding to logically merge potentially conflicting or overlapping visual descriptions (base protagonist look vs. agency items/roles).
*   Feeding a concatenated or structured prompt directly to the image model (Imagen) relies heavily on its internal interpretation capabilities, which can vary and lead to inconsistency.

## 3. Proposed Solution: Two-Step Prompt Generation

Implement a two-step process where an intermediary LLM call synthesizes the final, optimized prompt for the image generation model (Imagen):

1.  **Gather Inputs:** Collect the necessary visual components:
    *   `image_scene_description`: Generated by `ChapterManager.generate_image_scene()` (concise visual summary of the chapter's key moment).
    *   `protagonist_description`: The base visual description selected at the start (e.g., "Asian girl with long black hair wearing a blue dress"). Stored in `state.metadata['protagonist_description']`.
    *   `agency_details`: Structured agency info (name, category, and the *full* `visual_details` string like "[a grand owl...]"). Stored in `state.metadata['agency']`.
    *   `story_visual_sensory_detail`: The specific visual sensory detail selected for the chosen story category (e.g., "Aurora Dewdrops: Tiny orbs of light..."). Retrieved from `state.selected_sensory_details['visuals']`.

2.  **LLM Prompt Synthesis:**
    *   Create a new function (e.g., `synthesize_image_prompt`) that takes the above inputs.
    *   This function uses an LLM (LLM-Synthesizer, ideally a fast model like Gemini Flash) with a specific meta-prompt (detailed in Step 1 below).
    *   The meta-prompt instructs the LLM-Synthesizer to logically combine the inputs into a coherent, vivid prompt suitable for Imagen.

3.  **Image Generation:**
    *   Feed the synthesized prompt from Step 2 directly into `ImageGenerationService.generate_image_async()`.

## 4. Implementation Plan

**Prerequisite:**
*   [ ] **Define and Implement Protagonist Description Selection:**
    *   **File:** Potentially `app/services/chapter_manager.py` (during `initialize_adventure_state`) or a new utility.
    *   **Action:**
        *   Define a list of predefined protagonist descriptions.
        *   During adventure initialization, randomly select one description from the list.
        *   Store the selected description in `state.metadata['protagonist_description']`.
    *   **Predefined Protagonist Descriptions List:**
        ```python
        PREDEFINED_PROTAGONIST_DESCRIPTIONS = [
            "A curious young boy with short brown hair, bright green eyes, wearing simple traveler's clothes (tunic and trousers).",
            "An adventurous girl with braided blonde hair, freckles, wearing practical leather gear and carrying a small satchel.",
            "A thoughtful child with glasses, dark curly hair, wearing a slightly oversized, patched cloak over plain clothes.",
            "A nimble young person with vibrant red hair tied back, keen blue eyes, dressed in flexible, forest-green attire.",
            "A gentle-looking kid with warm brown eyes, black hair, wearing a comfortable-looking blue tunic and sturdy boots."
        ]
        ```
        *(This list should be placed in an appropriate constants file or within the ChapterManager)*

**Core Implementation Steps:**

*   [ ] **Step 1: Create Prompt Synthesis Function:**
    *   **File:** `app/services/image_generation_service.py` (or a new `prompt_synthesis_service.py`).
    *   **Function:** Define `async def synthesize_image_prompt(image_scene_description: str, protagonist_description: str, agency_details: dict, story_visual_sensory_detail: str) -> str:`
    *   **Meta-Prompt Design:** Craft the prompt for the LLM-Synthesizer. Key instructions:
        ```prompt
        # ROLE: Expert Prompt Engineer for Text-to-Image Models

        # CONTEXT:
        # You are creating an image prompt for one chapter of an ongoing children's educational adventure story (ages 6-12).
        # The image should be a snapshot of a key moment from this specific chapter.
        # The story features a main protagonist whose base look is described below.
        # The protagonist also has a special "agency" element (an item, companion, role, or ability) chosen at the start, with its own visual details.

        # INPUTS:
        # 1. Scene Description: A concise summary of the key visual moment in *this specific chapter*. **This scene takes priority.**
        #    "{image_scene_description}"
        # 2. Protagonist Base Look: The core appearance of the main character.
        #    "{protagonist_description}"
        # 3. Protagonist Agency Element: The special element associated with the protagonist.
        #    - Category: "{agency_details['category']}"
        #    - Name: "{agency_details['name']}"
        #    - Visuals: "{agency_details['visual_details']}"
        # 4. Story Sensory Visual: An overall visual mood element for this story's world. **Apply this only if it fits logically with the Scene Description.**
        #    "{story_visual_sensory_detail}"

        # TASK:
        # Combine these inputs into a single, coherent, vivid visual scene description (target 30-50 words).
        # Logically merge the Protagonist Base Look with the Agency Element Visuals. For example:
        #   - If Agency is "Guardian [gleaming silver armor]", describe the protagonist *wearing* the armor over their base clothes.
        #   - If Agency is "Wise Owl [snowy feathers...]", describe the protagonist *accompanied by* the owl within the scene.
        #   - If Agency is "Luminous Lantern [golden...]", describe the protagonist *holding* or *using* the lantern in the scene.
        #   - If Agency is "Element Bender [sparking flames...]", describe the protagonist *manifesting* these elements as part of the scene's action.
        # Integrate the combined character description naturally into the Scene Description.
        # The prompt MUST focus on what's happening *in the scene* while clearly including the protagonist and their agency element.
        # Prioritize the Scene Description: If the Story Sensory Visual detail contradicts the Scene Description (e.g., sensory detail mentions 'sparkling leaves at dawn' but the scene is 'inside a dark cave'), OMIT the sensory detail or adapt it subtly (e.g., 'glowing crystals line the cave walls' instead of 'sparkling leaves').

        # --- Examples of Prioritization ---
        # GOOD (Sensory fits Scene): Scene="Walking through a moonlit forest", Sensory="Glowing Juggling Pins", Output="...girl walks through a moonlit forest, juggling pins glow softly..."
        # GOOD (Sensory omitted): Scene="Inside a cozy tent", Sensory="Aurora Dewdrops on leaves", Output="...boy sits inside a cozy tent, reading a map..." (Dewdrops omitted as they don't fit).
        # BAD (Sensory forced): Scene="Inside a cozy tent", Sensory="Aurora Dewdrops on leaves", Output="...boy sits inside a cozy tent, strangely, there are dewdrops on leaves inside the tent..."
        # ---

        # The final output should be ONLY the synthesized prompt string itself, ready for an image model like Imagen.
        # Adopt a "colorful storybook illustration" style.

        # OUTPUT (Synthesized Prompt String Only):
        ```
    *   **LLM Call:** Use `LLMService` (configure to use the fastest model, e.g., Gemini Flash) via `generate_with_prompt` or a direct non-streaming call.
    *   **Error Handling:** Add try/except blocks. If synthesis fails, generate a fallback prompt (e.g., combine inputs with a simple template: `f"Colorful storybook illustration of this scene: {image_scene_description}. Protagonist: {protagonist_description}. Agency: {agency_details.get('visual_details', '')}. Atmosphere: {story_visual_sensory_detail}."`).
    *   **Logging:** Log all inputs and the final synthesized prompt.

*   [ ] **Step 2: Modify Image Generation Trigger Logic:**
    *   **File:** `app/services/websocket/image_generator.py`
    *   **Function:** Update `generate_chapter_image`.
    *   **Changes:**
        *   After getting `current_content`, call `image_scene = await chapter_manager.generate_image_scene(current_content)`.
        *   Retrieve `protagonist_description = state.metadata.get('protagonist_description', 'A young adventurer')` (add a default).
        *   Retrieve `agency_details = state.metadata.get('agency', {})`.
        *   Retrieve `story_visual_sensory_detail = state.selected_sensory_details.get('visuals', '')`.
        *   Call the new synthesis function: `synthesized_prompt = await synthesize_image_prompt(image_scene, protagonist_description, agency_details, story_visual_sensory_detail)`.
        *   Call the image generation service with the result: `task = asyncio.create_task(image_service.generate_image_async(synthesized_prompt))`
        *   Update the task list: `image_tasks.append(("chapter", task))`

*   [ ] **Step 3: Update `generate_agency_images` (Chapter 1):**
    *   **File:** `app/services/websocket/image_generator.py`
    *   **Function:** `generate_agency_images`.
    *   **Confirmation:** This step will *not* use the two-step synthesis process.
    *   **Action:** Modify the prompt generation within this function.
        *   For each choice `i`:
            *   Retrieve the specific agency choice `choice = chapter_content.choices[i]`.
            *   Find the `original_option` string (containing name and visual details) using `find_matching_agency_option`.
            *   Retrieve the `story_visual_sensory_detail = state.selected_sensory_details.get('visuals', '')`.
            *   Construct the prompt directly: `prompt = f"Colorful storybook illustration focusing on: {original_option}. Subtle atmosphere hints from: {story_visual_sensory_detail}."` (This focuses the image purely on the chosen agency element/companion, subtly influenced by the story's visual style).
            *   Create the task: `task = asyncio.create_task(image_service.generate_image_async(prompt))`
            *   Append: `image_tasks.append((i, task))`

*   [ ] **Step 4: Remove/Refactor Redundant Code:**
    *   **File:** `app/services/image_generation_service.py`
    *   **Function:** `enhance_prompt`.
    *   **Action:** This function is no longer needed for chapter images and can be removed or significantly simplified if only used for potential future fallback scenarios. The core logic is now in `synthesize_image_prompt`.

*   [ ] **Step 5: Configuration & Model Selection:**
    *   Ensure the LLM call within `synthesize_image_prompt` uses the intended fast model (e.g., Gemini Flash).

*   [ ] **Step 6: Logging and Monitoring:**
    *   Add detailed logs within `synthesize_image_prompt`.
    *   Monitor latency.

*   [ ] **Step 7: Thorough Testing:**
    *   Run unit tests for `synthesize_image_prompt`.
    *   Run integration tests using `generate_all_chapters.py`, logging intermediate and final prompts.
    *   Perform extensive visual inspection of generated images across multiple chapters and stories, checking for protagonist appearance, agency integration, scene relevance, and appropriate application of sensory details.

## 5. Potential Challenges & Risks

*   **Latency:** Adding an extra LLM call *will* increase the time it takes for an image to appear. Mitigation: Use the fastest possible LLM for synthesis, optimize the meta-prompt.
*   **Cost:** Doubling the LLM calls per image increases operational costs.
*   **Synthesizer Reliability:** The quality depends on the LLM-Synthesizer correctly interpreting the meta-prompt. Errors in synthesis will propagate.
*   **Meta-Prompt Engineering:** Crafting the meta-prompt requires careful design and testing.