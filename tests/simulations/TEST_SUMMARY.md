# Simulation Tests Summary

> **DEPRECATED**: This document has been merged into [SIMULATION_TESTING.md](./SIMULATION_TESTING.md), which now contains all simulation and testing documentation. Please refer to the new document for the most up-to-date information.

This document provides an overview of the tests for the story simulation system, focusing on error detection, logging, and recovery mechanisms.

## Test Files

- `test_simulation_errors.py`: Tests for error detection, logging, and recovery in simulation logs
- `test_story_simulation.py`: Tests for the story simulation functionality

## Error Tests Overview

The `test_simulation_errors.py` file contains tests that verify the simulation's ability to handle errors gracefully and continue operation. These tests are designed to run against simulation log files, such as those generated by `story_simulation.py`.

### Test: Error Detection and Classification

**Purpose**: Verifies that errors in the simulation log are properly detected and classified.

**What it checks**:
- Errors in the log file are correctly identified
- Errors can be classified into different categories (connection, service, etc.)
- The simulation completes successfully despite encountering errors

**If it fails**:
- Check if the log file format has changed
- Verify that the error detection function can parse the log format
- Ensure the simulation is completing with a STORY_COMPLETE event

### Test: Logging Level Sufficient for Errors

**Purpose**: Verifies that the logging level in story_simulation.py is sufficient to print errors.

**What it checks**:
- ERROR level messages are present in the log file
- Specific error types (like WebSocket connection errors) are being logged
- The logging level in story_simulation.py is set to capture ERROR messages

**If it fails**:
- Check if the logging level in story_simulation.py has been changed
- Verify that the logger is properly configured to log ERROR level messages
- Ensure that error handling code is using the logger to log errors

### Test: Error Recovery Mechanism

**Purpose**: Verifies that the simulation can recover from certain types of errors.

**What it checks**:
- The retry mechanism is working after errors occur
- The simulation can continue and complete successfully after encountering errors
- Retry messages are properly logged

**If it fails**:
- Check if the retry logic in story_simulation.py is working correctly
- Verify the MAX_RETRIES and RETRY_DELAY constants
- Ensure that WebSocket reconnection is implemented properly
- Check that error handling doesn't terminate the simulation prematurely

### Test: Comprehensive Error Analysis

**Purpose**: Performs a comprehensive analysis of errors in the simulation log.

**What it checks**:
- Errors can be mapped to specific chapters in the simulation
- The simulation makes significant progress despite errors (at least 80% completion)
- Chapter information can be extracted from the log file

**If it fails**:
- Check if the chapter information format in the log has changed
- Verify that the simulation is making sufficient progress despite errors
- Ensure that chapter events are being properly logged
- Check if the expected story length (10 chapters) has changed

### Test: No Critical Errors

**Purpose**: Verifies that there are no critical errors in the simulation log.

**What it checks**:
- Errors can be classified as critical or recoverable
- No critical errors (fatal, crash, exception, traceback) are present in the log
- The simulation completes successfully despite recoverable errors

**If it fails**:
- Check for critical error keywords in the log (fatal, crash, exception, traceback)
- Investigate any critical errors found and fix their root causes
- Ensure that error handling is properly categorizing errors

### Test: Error Logging in Story Simulation

**Purpose**: Verifies that the story_simulation.py file has appropriate error logging.

**What it checks**:
- The story_simulation.py file contains error logging statements
- There are try-except blocks to catch and handle errors
- Error handling is implemented throughout the code

**If it fails**:
- Check if error logging statements have been removed from story_simulation.py
- Verify that try-except blocks are present to catch errors
- Ensure that error handling is properly implemented

## Running the Tests

To run all error tests:
```bash
pytest tests/simulations/test_simulation_errors.py
```

To run a specific test:
```bash
pytest tests/simulations/test_simulation_errors.py::test_error_recovery_mechanism
```

## Test Dependencies

These tests depend on:
- A valid simulation log file (either specified by run_id or the latest log)
- The story_simulation.py file with proper error logging
- The log_utils.py module for log file parsing and analysis

## Expected Output

When tests pass, you should see output similar to:
```
========================== 6 passed in 0.33s ==========================
```

If a test fails, the error message will indicate which assertion failed and provide guidance on how to fix the issue.
