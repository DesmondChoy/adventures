# Simulation and Testing Guide

This document provides a comprehensive overview of the simulation and testing system for the Learning Odyssey application.

## Table of Contents

1. [Introduction](#introduction)
2. [Simulation Components](#simulation-components)
3. [Test Suites](#test-suites)
4. [Usage Guide](#usage-guide)
5. [Implementation Details](#implementation-details)
6. [Troubleshooting](#troubleshooting)

## Introduction

The simulation and testing system serves multiple purposes:

### Dual Purpose of Simulations

1. **Primary: Data Generation Tool**
   - Produces comprehensive logs with standardized prefixes
   - Captures the complete state transitions throughout a user journey
   - Generates consistent output that can be analyzed by dedicated test files

2. **Secondary: End-to-End Verification**
   - Verifies that the complete workflow can execute successfully
   - Acts as a basic smoke test for the integrated system
   - Validates that all components can work together

### Testing Approach

The testing approach is based on analyzing the logs generated by simulations:
- Tests verify specific behaviors and requirements
- Multiple test suites can use the same simulation output
- Standardized log prefixes enable consistent parsing
- Tests can search for specific patterns in the logs
- State transitions can be reconstructed from the log data
- Run ID tracking ensures test isolation

## Simulation Components

### Key Files

- `story_simulation.py`: A Python script that simulates user interaction with the story generation WebSocket API
- `log_utils.py`: Utility functions for finding, parsing, and analyzing simulation logs
- `test_simulation_functionality.py`: Tests for the functional correctness of the simulation system
- `test_simulation_errors.py`: Tests for error detection, logging, and recovery in simulation logs

### Logging System

The simulation uses a sophisticated logging system:

- **Timestamped Log Files**: Each simulation run creates its own log file with timestamp and unique run ID
  - Format: `logs/simulations/simulation_YYYY-MM-DD_HH-MM-SS_runid.log`
  - Prevents log file confusion and makes it easy to identify specific runs
  
- **Run Metadata**: Each log file includes complete run metadata:
  - Run ID (unique identifier for each simulation run)
  - Timestamp (when the simulation was started)
  - Story category and lesson topic
  - Story length
  - Duration (for completed runs)
  
- **Run Boundaries**: Clear START and END markers in logs:
  - `SIMULATION_RUN_START` - Marks the beginning of a simulation run with metadata
  - `SIMULATION_RUN_END` - Marks the successful completion of a run with summary statistics

- **Structured JSON Logging**: Uses the application's `StructuredLogger` for consistent format
  - All log entries include the run ID for easy filtering
  - JSON-formatted metadata for automated parsing

- **Standardized Event Prefixes**:
  - `EVENT:CHAPTER_START` - Logs the start of a new chapter with type information
  - `EVENT:CHOICES_PRESENTED` - Logs when choices are presented to the user
  - `EVENT:CHOICE_SELECTED` - Logs user choice selections
  - `EVENT:LESSON_QUESTION` - Logs lesson question details
  - `EVENT:LESSON_ANSWER` - Logs lesson answer correctness
  - `EVENT:STORY_COMPLETE` - Logs story completion with statistics
  - `EVENT:PROCESS_START` - Logs the start of a process like `process_consequences`

### Utility Functions

The `log_utils.py` module provides several utility functions:

- **Log Finding Functions**:
  - `get_latest_simulation_log()` - Get the most recent simulation log
  - `get_simulation_logs_by_date(date_str)` - Find logs from a specific date
  - `get_simulation_log_by_run_id(run_id)` - Find a specific run by ID
  - `find_simulations_by_criteria(...)` - Find runs matching specific criteria

- **Log Parsing Functions**:
  - `parse_simulation_log(log_file)` - Extract structured data from a log file
  - `get_chapter_sequence(log_file)` - Get the sequence of chapter types
  - `count_lesson_chapters(log_file)` - Count the number of lesson chapters
  - `get_lesson_success_rate(log_file)` - Calculate lesson success rate
  - `check_simulation_complete(log_file)` - Check if a simulation completed successfully
  - `get_simulation_errors(log_file)` - Get all errors from a simulation run

## Test Suites

### Functionality Tests (`test_simulation_functionality.py`)

This test file verifies the functional correctness of the simulation system by analyzing simulation logs. It includes tests for:
- Chapter sequence verification (ensuring proper STORY/LESSON/CONCLUSION ordering)
- Lesson ratio validation (approximately 50% of flexible chapters)
- Success rate calculation for lesson questions
- Simulation metadata verification
- State transition consistency

### Error Handling Tests (`test_simulation_errors.py`)

This test file verifies the simulation's ability to handle errors gracefully and continue operation. The tests are designed to automatically skip if no errors are found in the log file, making them compatible with both error-containing and error-free simulation runs.

#### Test: Error Detection and Classification

**Purpose**: Verifies that errors in the simulation log are properly detected and classified.

**What it checks**:
- Errors in the log file are correctly identified
- Errors can be classified into different categories (connection, service, etc.)
- The simulation completes successfully despite encountering errors

**Note**: This test will be skipped if no errors are found in the log file.

**If it fails**:
- Check if the log file format has changed
- Verify that the error detection function can parse the log format
- Ensure the simulation is completing with a STORY_COMPLETE event

#### Test: Logging Level Sufficient for Errors

**Purpose**: Verifies that the logging level in story_simulation.py is sufficient to print errors.

**What it checks**:
- ERROR level messages are present in the log file
- Specific error types (like WebSocket connection errors) are being logged
- The logging level in story_simulation.py is set to capture ERROR messages

**Note**: This test will be skipped if no ERROR level messages or WebSocket connection errors are found in the log file.

**If it fails**:
- Check if the logging level in story_simulation.py has been changed
- Verify that the logger is properly configured to log ERROR level messages
- Ensure that error handling code is using the logger to log errors

#### Test: Error Recovery Mechanism

**Purpose**: Verifies that the simulation can recover from certain types of errors.

**What it checks**:
- The retry mechanism is working after errors occur
- The simulation can continue and complete successfully after encountering errors
- Retry messages are properly logged

**Note**: This test will be skipped if no retry messages are found in the log file.

**If it fails**:
- Check if the retry logic in story_simulation.py is working correctly
- Verify the MAX_RETRIES and RETRY_DELAY constants
- Ensure that WebSocket reconnection is implemented properly
- Check that error handling doesn't terminate the simulation prematurely

#### Test: Comprehensive Error Analysis

**Purpose**: Performs a comprehensive analysis of errors in the simulation log.

**What it checks**:
- Errors can be mapped to specific chapters in the simulation
- The simulation makes significant progress despite errors (at least 80% completion)
- Chapter information can be extracted from the log file

**Note**: This test will be skipped if no errors are found in the log file.

**If it fails**:
- Check if the chapter information format in the log has changed
- Verify that the simulation is making sufficient progress despite errors
- Ensure that chapter events are being properly logged
- Check if the expected story length (10 chapters) has changed

#### Test: No Critical Errors

**Purpose**: Verifies that there are no critical errors in the simulation log.

**What it checks**:
- Errors can be classified as critical or recoverable
- No critical errors (fatal, crash, exception, traceback) are present in the log
- The simulation completes successfully despite recoverable errors

**Note**: This test will be skipped if no errors are found in the log file.

**If it fails**:
- Check for critical error keywords in the log (fatal, crash, exception, traceback)
- Investigate any critical errors found and fix their root causes
- Ensure that error handling is properly categorizing errors

#### Test: Error Logging in Story Simulation

**Purpose**: Verifies that the story_simulation.py file has appropriate error logging.

**What it checks**:
- The story_simulation.py file contains error logging statements
- There are try-except blocks to catch and handle errors
- Error handling is implemented throughout the code

**If it fails**:
- Check if error logging statements have been removed from story_simulation.py
- Verify that try-except blocks are present to catch errors
- Ensure that error handling is properly implemented

## Usage Guide

### Prerequisites

1. The FastAPI application must be running:
   ```bash
   uvicorn app.main:app --reload
   ```

2. Required Python packages:
   - websockets
   - asyncio
   - json
   - yaml
   - pandas
   - logging
   - pytest (for running tests)

### Orchestration Script

The `run_simulation_tests.py` script automates the entire testing workflow by:

1. Starting the FastAPI server with uvicorn
2. Running the story simulation
3. Executing all pytest tests in tests/simulations with prefix test_
4. Handling proper cleanup of all processes

#### Using the Orchestration Script

Execute the script from the project root directory:
```bash
python tests/simulations/run_simulation_tests.py
```

#### Command-Line Options

The orchestration script supports several command-line arguments:

```bash
# Run with specific story category and lesson topic
python tests/simulations/run_simulation_tests.py --category "enchanted_forest_tales" --topic "Farm Animals"

# Skip simulation and just run tests on existing logs
python tests/simulations/run_simulation_tests.py --tests-only

# Run only the simulation without tests
python tests/simulations/run_simulation_tests.py --simulation-only

# Use a different port for the server
python tests/simulations/run_simulation_tests.py --server-port 8080

# Use a different host for the server
python tests/simulations/run_simulation_tests.py --server-host "0.0.0.0"
```

### Running the Simulation Manually

If you prefer to run the simulation manually without the orchestration script:

```bash
python tests/simulations/story_simulation.py
```

#### Simulation Command-Line Options

The simulation script supports command-line arguments:

```bash
# Get just the run ID (useful for test scripts)
python tests/simulations/story_simulation.py --output-run-id

# Specify a story category
python tests/simulations/story_simulation.py --category "enchanted_forest_tales"

# Specify a lesson topic
python tests/simulations/story_simulation.py --topic "Farm Animals"
```

### Running Tests Manually

After running a simulation, you can run the tests manually to analyze the output:

```bash
# Run all functionality tests
pytest tests/simulations/test_simulation_functionality.py

# Run all error handling tests
pytest tests/simulations/test_simulation_errors.py

# Run a specific test
pytest tests/simulations/test_simulation_functionality.py::test_chapter_sequence
pytest tests/simulations/test_simulation_errors.py::test_error_recovery_mechanism

# Run tests with verbose output
pytest -v tests/simulations/test_simulation_functionality.py
pytest -v tests/simulations/test_simulation_errors.py
```

### Expected Output

When tests pass, you should see output similar to:
```
========================== 6 passed in 0.33s ==========================
```

If a test fails, the error message will indicate which assertion failed and provide guidance on how to fix the issue.

### Monitoring Output

- The script will randomly select a story category and lesson topic (story length is fixed at 10 chapters)
- It will connect to the WebSocket endpoint and simulate a complete story adventure
- Chapter content will be displayed in the terminal
- Choices will be randomly selected at each decision point
- For lesson chapters, the script will log whether the selected answer was correct
- At the end, statistics about the story completion will be displayed

### Viewing Logs

The simulation logs detailed information to:
- Console (INFO level and above)
- Timestamped log file (DEBUG level and above): `logs/simulations/simulation_YYYY-MM-DD_HH-MM-SS_runid.log`

To view the latest simulation log:
```bash
# Using the log_utils.py module
python -c "from tests.simulations.log_utils import get_latest_simulation_log; print(get_latest_simulation_log())"

# View the content of the latest log
cat $(python -c "from tests.simulations.log_utils import get_latest_simulation_log; print(get_latest_simulation_log())")
```

## Implementation Details

### Connection Management

The simulation establishes a WebSocket connection to the server using the following URL pattern:
```
ws://localhost:8000/ws/story/{story_category}/{lesson_topic}
```

The script includes retry logic with exponential backoff to handle connection failures:
- Maximum 3 retry attempts
- 2-second delay between retries
- Proper cleanup on connection failure

### State Management

The simulation initializes and maintains state according to the AdventureState model:

1. Initial state message:
   ```json
   {
     "state": {
       "current_chapter_id": "start",
       "story_length": 10,  // Fixed at 10 chapters
       "chapters": [],
       "selected_narrative_elements": {},
       "selected_sensory_details": {},
       "selected_theme": "",
       "selected_moral_teaching": "",
       "selected_plot_twist": "",
       "planned_chapter_types": [],
       "current_storytelling_phase": "Exposition",
       "metadata": {}
     }
   }
   ```

2. Choice messages:
   ```json
   {
     "state": <current_state>,
     "choice": {
       "chosen_path": <choice_id>,
       "choice_text": <choice_text>
     }
   }
   ```

### Response Handling

The simulation handles different types of responses from the server:

1. **Text Content**: Non-JSON responses are treated as story content and accumulated for display.

2. **Chapter Updates**: JSON responses with `type: "chapter_update"` are used to update the state.
   ```json
   {
     "type": "chapter_update",
     "state": {
       "current_chapter_id": <chapter_id>,
       "current_chapter": {
         "chapter_number": <number>,
         "content": <content>,
         "chapter_type": <type>,
         "chapter_content": { ... },
         "question": <question_data>
       }
     }
   }
   ```

3. **Choices**: JSON responses with `type: "choices"` provide options for the user to select.
   ```json
   {
     "type": "choices",
     "choices": [
       { "text": <choice_text>, "id": <choice_id> },
       ...
     ]
   }
   ```

4. **Story Complete**: JSON responses with `type: "story_complete"` indicate the end of the story.
   ```json
   {
     "type": "story_complete",
     "state": {
       "current_chapter_id": <chapter_id>,
       "stats": {
         "total_lessons": <count>,
         "correct_lesson_answers": <count>,
         "completion_percentage": <percentage>
       }
     }
   }
   ```

### Chapter Type Handling

The simulation detects and handles different chapter types:

1. **Story Chapters**: Chapters with `chapter_type: "story"` present narrative choices.
   - The simulation randomly selects one of the available choices.
   - The choice is sent back to the server to progress the story.

2. **Lesson Chapters**: Chapters with `chapter_type: "lesson"` present educational questions.
   - The simulation identifies the correct answer for logging purposes.
   - It randomly selects an answer and logs whether it was correct.
   - The choice is sent back to the server to progress the story.

3. **Conclusion Chapters**: Chapters with `chapter_type: "conclusion"` end the story.
   - The simulation displays the final content and statistics.
   - No further choices are presented.

### Error Handling

The simulation includes comprehensive error handling:

1. **Connection Errors**: Retry logic with exponential backoff.
2. **Timeout Errors**: Configurable timeout (30 seconds) for waiting for responses.
3. **JSON Parsing Errors**: Fallback to treating responses as raw text.
4. **WebSocket Closure**: Proper cleanup of resources.
5. **Signal Handling**: Graceful shutdown on SIGINT and SIGTERM.

## Troubleshooting

Common issues and solutions:

### Orchestration Script Issues
- **Server Startup Failures**: 
  - Check if the port is already in use
  - Verify that uvicorn is installed
  - Check for any error messages in the server output
- **Process Cleanup Issues**:
  - If processes are left running after script termination, check for error messages
  - Manually terminate any orphaned processes using `ps` and `kill` commands
  - Ensure the script has proper permissions to terminate processes

### Connection Failures
- Ensure the FastAPI server is running
- Check the WebSocket URL format
- Verify network connectivity

### Story Generation Errors
- Check LLM service configuration
- Ensure sufficient lesson questions are available
- Verify story category and lesson topic exist in data files

### State Inconsistencies
- Check for mismatches between client and server state
- Verify the state structure matches AdventureState model
- Ensure chapter progression is properly tracked

### Log File Issues
- Ensure the logs/simulations directory exists
- Check file permissions
- Verify that the simulation script has write access to the logs directory

### Test Failures
- Check if the log file format has changed
- Verify that the error detection function can parse the log format
- Ensure the simulation is completing with a STORY_COMPLETE event
- Check if the logging level in story_simulation.py has been changed
- Verify that the logger is properly configured to log ERROR level messages

## Extending the System

To extend the simulation and testing system:

1. **Custom Story Selection**: Modify the random selection functions to target specific stories or topics.
2. **Deterministic Choices**: Replace random choice selection with predetermined choices for reproducible testing.
3. **Performance Testing**: Add timing measurements for response latency analysis.
4. **Error Injection**: Modify messages to test error handling in the server.
5. **Parallel Simulations**: Run multiple instances to test server load handling.
6. **Custom Test Cases**: Create new test files that use the log_utils.py module to test specific behaviors.
7. **Additional Error Tests**: Add more specialized tests for specific error conditions.
