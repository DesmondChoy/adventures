---
description: Run story_simulation.py on relevant code changes for debugging
globs: app/services/llm/*, app/routers/websocket.py, app/services/chapter_manager.py, app/models/story.py, app/init_data.py, app/data/stories.yaml, app/data/lessons.csv
---
# Run Story Simulation for Debugging

This rule automatically executes `tests/simulations/story_simulation.py` whenever you modify files related to story generation, LLM interactions, WebSocket communication, or story data. This helps you quickly test and debug your changes by observing the story simulation output.

<rule>
name: run_story_simulation_on_change
description: Execute story_simulation.py after changes in story related files
filters:
  - type: file_path
    pattern: "^(app/services/llm/.*|app/routers/websocket\\.py|app/services/chapter_manager\\.py|app/models/story\\.py|app/init_data\\.py|app/data/stories\\.yaml|app/data/lessons\\.csv)$"
  - type: event
    pattern: "file_change|file_create|file_delete"

actions:
  - type: shell_command
    command: "source .venv/bin/activate && python tests/simulations/story_simulation.py"
    background: false # Set to false to see output in terminal immediately
    message: |
      Running story simulation `tests/simulations/story_simulation.py` due to changes in story-related files.

      **Debug Workflow:**

      1. **Review the Output:** The simulation output (LLM prompts, LLM responses, debug logs) is printed directly to your terminal.

      2. **Assess Changes:**
         - **Expected Behavior?** Verify if the prompts generated and the story flow are as you intended after your code changes.
         - **Errors or Issues?** Check for any errors in the simulation output or unexpected story behavior.

      3. **Iterate if Necessary:**
         - **Unexpected Output or Errors:** If the simulation reveals issues or the behavior is not as expected, continue debugging your code.
         - **Re-run Simulation:** After fixing issues, modify your code further and save the file again. This rule will automatically re-run `story_simulation.py`.
         - **Rinse and Repeat:** Continue this cycle of code changes, simulation runs, and debugging until the simulation behaves as desired.

      By using this rule, you can efficiently debug and validate changes to the story generation logic and ensure that the application behaves as expected after your modifications.

examples:
  - input: "Modified app/services/llm/prompt_engineering.py"
    output: "Story simulation started. Review terminal output for prompts, responses, and debug logs."

metadata:
  priority: medium
  version: 1.0
</rule>

